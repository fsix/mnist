{
  "name": "MNIST",
  "tagline": "Tips and Tricks for Cracking ML's Favorite Image Dataset",
  "body": "# Introduction\r\n\r\nMNIST is a dataset of handwritten digits published in the 1990s, MNIST is perhaps one of the most iconic exercises for beginning Machine Learning, and a milestone in using computers to structurally analyse images. This guide will serve as an exploration of the MNIST dataset, beginning with simple methods and eventually reaching methods that will reach 99% accuracy.\r\n\r\n## Preprocessing Techniques\r\n\r\n### Deskewing\r\n\r\nDeskewing images involves offsetting and skewing images so their center of masses coincide with the center of the image and such that the x and y covariance of the pixel intensities is 0. By reversing the proposed affine transformation, we essentially \"standardize\" the images, making it much easier for the ML algorithm to distinguish between images.\r\n\r\n[Check out the Jupyter Notebook here](Deskewing.html)\r\n\r\n## Optimizing Hyperparameters\r\n\r\n\r\n\r\n###  Line/Grid Search \r\n\r\n### Bayesian Optimization \r\n\r\nThis was too complicated to explain in a single iPython notebook, so we'll release this later.\r\n\r\n\r\n## Adverserial Training\r\n\r\n### Perturbation\r\n\r\nPerturbation is the technique of generating \"jittered\" images from our original training dataset. In particular, we found that adding random noise, random rotations, skews, and elastic distortions help generalize our dataset against adversarial images.\r\n\r\nThis technique is a specialized form of training for \"adversarial images\", when the testing metric is designed to challenge and perform the worst on overfit and nongeneralized models. \r\n\r\n[Check out the Jupyter Notebook here](Perturbation.html)\r\n\r\n## Guiding Approximations\r\n\r\n### Momentum and Annealing for SGD",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}